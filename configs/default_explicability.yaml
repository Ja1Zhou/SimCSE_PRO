model_args:
  model_name_or_path: bert-base-uncased
  temp: 0.05
  pooler_type: avg
  hard_negative_weight: 0
  do_mlm: false
  mlm_weight: 0.1
  mlp_only_train: false
  freeze_backbone: false
  add_entropy_loss: false
  model_class: SelfWeightedBertForCL
  model_class_args:
    - config
    - model_args
  model_forward_args:
    - output_hidden_states
    - return_dict
    - sent_emb
  forward_function: self_weighted
  init_function: default_init
  eval_transfer: false
data_args:
  learning_style: unsup
  train_file: data/wiki1m_for_simcse.txt
  max_seq_length: 32
  pad_to_max_length: false
  mlm_probability: 0.15
  force_unsup: false
  batch_size: 128
  preprocessing_num_workers: 4
  overwrite_cache: false
  dataset_map_function: default
  dataset_map_function_args:
    - tokenizer
    - data_args
    - sent0_cname
    - sent1_cname
    - sent2_cname
  custom_collator: true
  collator: get_default_custom
  collator_args:
    - tokenizer
    - model_args
    - data_args
training_args:
  output_dir: "result/${data_args.learning_style}-${model_args.model_name_or_path}-${model_args.pooler_type}-${model_args.model_class}-${model_args.forward_function}"
  num_train_epochs: 1
  per_device_train_batch_size: 64
  learning_rate: 3.e-5
  evaluation_strategy: steps
  eval_steps: 125
  logging_steps: ${training_args.eval_steps}
  metric_for_best_model: stsb_spearman
  load_best_model_at_end: true
  overwrite_output_dir: true
  do_train: true
  do_eval: true
  fp16: true
trainer_args:
  optimizer_args:
    custom_optimizer: true
    optimizer_class: AdamW
    optimizer_class_args:
      - params: model.bert.parameters()
        lr: "${training_args.learning_rate}"   
      - params: model.get_mask.parameters()
        lr: ${training_args.learning_rate} * 10       
    scheduler: get_linear_schedule_with_warmup
    scheduler_warmup_steps: 0
  trainer_class: CLTrainer
  trainer_class_args:
    - model
    - args
    - train_dataset
    - tokenizer
    - data_collator
    - optimizers
    - model_args
    - trainer_args
  batcher_function: default_batcher
  batcher_function_args:
    - tokenizer
    - model_args
    - training_args
    - model
explicability_args:
  input_sentences:
    - "BERT is a model with absolute position embeddings so itâ€™s usually advised to pad the inputs on the right rather than the left."
    - "The dog is running across the dirty path."
    - "It is a paradox of the Victorians that they are both insular and throughout the empire, cosmopolitan."
  output_dir: explicability
defaults:
  - _self_
  - override hydra/job_logging: custom_explicability
hydra:
  job:
    name: "logs/eval-${data_args.learning_style}-${model_args.model_name_or_path}-${model_args.pooler_type}-${model_args.forward_function}"
  run:
    dir: .
  output_subdir: null